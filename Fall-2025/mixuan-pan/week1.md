
State: I am not stuck with anything, don't need help right now. 

Progress: Studied the MIT notes. 

< insert more progress here > if asked to submit notes, then submit the notes here. 

# Notes 
https://ocw.mit.edu/courses/6-004-computation-structures-spring-2017/pages/

## Ch.7 Performance Measures 

- Latency tpd: The delay from when an input is established until the output associated with that input becomes valid 
- Throughput 1/tpd: The rate at which the inputs or outputs processed (1/latency) 
- Tradeoff: piplined processing -> increase throughput -> all pipeline stages must operate in lock-step -> the rate of processing is determined by the slowest stage -> latency increases 

- the total elapsed time form valid input to valid output is determined by the propagation delays of the component modules -> can't improve latency 

### Pipelined Circuits 
- use registers to hold the values while other parts working on the next cycle 
- the appropriate clock period for this sequential circuit is determined by the propagation delay of the slowest processing stage 
- increase the throughput of combinational logic: use registers to divide the processing into a sequence of stages, where the registers capture the outputs from one processing stage and hold them as inputs for the next processing stage.
- a particular input will progress through the system at the rate of one stage per clock cycle 
- the pipeline system has better throughput at the cost of a small increase in latency 

### Pipeline Convention 
- DEFINITION: A well formed K-staged pipeline (K-pipeline) is an acyclic circuit having exactly K registers on "every" path from an input to an output
- -> a combinational circuit is an 0-stage pipleine 
- COMPOSITION CONVENTION: every pipleine stage, hence every K-stage pipeline, has a register o nits "OUTPUT"
- ALWAYS: the CLOCK common to all registers must have a period sufficient to cover porpagation over combinational paths PLUS (input) register tPD PLUS (output) register tSETUP 
- The LATENCY of a K-pipeline is K x the period of a system's clock 
the THROUGHPUT of a K-pipeline is the frequency of the clk (1/clk period)

### III-formed Pipelines 
- failed pipeline: some paths form inputs to outputs have 2 registers, and some have only 1 

### Pipelining Methodology 
- Strategy: Focus on placing pipelinging registers around the slowest circuit elements (BOTTLENECKS) 
- Step 1: Draw a line that rosses every output in the circuit and mark the endpoints as terminal points 
- Step 2: Continue to draw new lines between the terminal points across various circuit connections, ensuring that every connection crosses each line in the same direction. These lines demarcate "pipline stages" 
- Adding a pipline register at every point where a separating line crosses a connection will always generate a valid pipeline 

### Pipelined Components 
- hierarchical pipelined systems: replace a slow combinational component with a k-pipe version -> decrease the clk period
- must accoutn for new pipeline stages 
- replace the original component with a k-staged pipelined version 

### Bottleneck - improve throughput 
- replace the component with a pipelined versino 
- interleave multiple copies of the components 

### Circuit Interleaving 
- simulate a pipelined version of a slow compoenent by replicating the critical element and alternate inputs between various copies 

| Timed | Synchronous | Asynchronous |
| :--- | :--- | :--- |
| Globally Timed | Centralized clocked FSM generates all control signals - easy to design but fixed-sized interval can be wasteful (no data dependencies in timing| Centro lcontrol unit tailors current time slice to current tasks - large systems lead to very complicated timing generators (NO) | 
| Locally Timed | Start and Finish signals generated by each major sybsystem, synchronously with global clk - the best way to build large systems that have independently timed components | each subsystem takes asynchronous finish (perhaps using local clk) = the "next big idea" for the last severeal decades: a lot of design work to do in general, but extra work is worth it in special cases |

## Ch. 14 Caches and the Memory Hierarchy 

| | Capacity | Latency | Cost/GB | Category |
| :---: | :--: | :---: | :---: | :---: |
| Register | 1000s of bits | 20ps | $$$$ | Processor Datapath | 
| SRAM | 10KB - 10MB | 1-10ns | $1000 | Memory Hierarchy | 
| DRAM | 10GB | 80ns | $10 | 
| Flash* | 100GB | 100us | $1 | I/O subsystem | 
| Hard disk* | 1TB | 10ms | $0.1 |

- * non-volatile (retains contents when powered off) 

### Static Ram (SRAM) 
- organized as an array of memory locations, where a memory accs is either reading or writing all the bits in a single location. 
- circuitry around the periphery is used to decode addresses and support read & write operations 
- To access the SRAM, we need to provide enough address bits to uniquely specify the location
- The active wordline enables each of the SRAM bit cells on the selected row, connecting each cell to a pair of bit lines (the vertical wires in the array) 
- During read operations the bit lines carry the analog signals from the enabled bit cells to the sense amplifiers ->  convert the analog signals to digital data.
- During write operations incoming data is driven onto the bit lines to be stored into the enabled bit cells.
- Larger SRAMs will have a more complex organization in order to minimize the length, and hence the capacitance, of the bit lines.

### SRAM Cell 
- 2 CMOS inverters (4 MOSFETS) -> bistable element 
- 2 accees transistors 

### Why Caches Help
- **Locality**:  
  - *Temporal*: recently used items likely reused.  
  - *Spatial*: nearby addresses likely accessed.
- Programs’ **working sets** often fit in small, fast storage → big speedups.

### Average Memory Access Time (AMAT)
- `AMAT = HitTime + MissRatio × MissPenalty`
- For multi-level caches, apply recursively (use L1’s miss as L2’s access, etc.).

### Cache Organization
- Address → **Tag | Index | BlockOffset**.
- **Direct-Mapped (DM)**: 1 line per index; simple/fast but conflict-prone.
- **Fully-Associative (FA)**: any block anywhere; minimal conflicts, expensive tag search.
- **N-way Set-Associative (SA)**: N lines per set; good trade-off.

### Block (Line) Size Trade-offs
- Larger blocks exploit spatial locality (↓misses) but: ↑miss penalty, ↓number of lines (pressure on working set).
- Choose via AMAT experiments (common modern default ~64B).

### Replacement Policy
- **LRU** (or approximations) good locality match but costs state/logic.
- **FIFO** and **Random** are simpler/cheaper; Random avoids some adversarial cases.

### Write Policy
- **Write-Through**: update memory on every write (simple, bandwidth-heavy).
- **Write-Back** (+ dirty bits): update memory on eviction (bandwidth-efficient; widely used).

## Ch. 15 Pipelining the Beta

### 5-Stage Pipeline
- **IF** (fetch) → **RF** (read regs) → **EX/ALU** → **MEM** (data access) → **WB** (write-back).  
- Ideal CPI ≈ 1 (no hazards, balanced stages); period set by slowest stage.

### Data Hazards (RAW) & Remedies
- **Stall**: hold IF/RF until producer writes result (simple, hurts throughput).
- **Bypass/Forwarding**: route results from later stages to earlier stage inputs; removes most stalls.
- **Load-to-Use**: even with full bypassing, a use immediately after a `LD` typically needs a 1-cycle stall (data ready late).

### Control Hazards (Branches/Jumps)
- **Predict not-taken (PC+4)** baseline; on mispredict, **flush/annul** younger instructions.
- Alternatives:  
  - **Stall** until branch resolution (simple, slow),  
  - **Delay slots** (ISA-level—historical),  
  - **Prediction & speculation** (best performance when accurate).

### Exceptions & Interrupts in a Pipeline
- Capture faulting PC (or PC+4 as defined by ISA), **annul** younger instructions, and vector to handler.
- Integrate with control-hazard machinery so behavior matches the unpipelined architecture.
