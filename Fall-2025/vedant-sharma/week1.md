Design Log Week 1:

I am not stuck at this point of time.

-----------------------------------------------------------------

Progress: 

I am not stuck, however, I do need assistance with brightspace as of right now. I have emailed Johnathan about the issue and am currently waiting to hear back from him about this issue.

This week is intro week, we have just decided sub-teams through the AI Hardware team and I will be working on the vector core subteam, my role is verification, so I will be creating testbenches verifying if the RTL code we are writing for components are properly working as expected. My goal during this time is to learn and absorb as much knowledge as possible in regards to the digital design of these components while gaining an understanding as to how they work, while simultaneously building up my skills in verification for industry roles. As of right now I am unable to settle down on certain metrics I should target for, but I will update as I learn more about the team and my role.

To start off, I must read and gain a better comprehensive understanding of AI hardware and in specific vector processing. This weeks log will contain notes from MIT's computation structures course, the chapters being captured are 7, 14, and 15.

-------------------------------------------------------------------

Chapter 7: This chapter is on performance measures. The 7.1 annotated slides talk initially talk about "pipelining" where after the first cycle where the washer gets used, when the clothes are moved to the dryer the empty washer gets used for the next set of dirty clothes, this way overall throughput is faster. This is the same example that was shown in the focus group. They also cover the fact that there are delays that are determined based off the slowest part of the pipeline process. So essentially there is a latency increase as certain process have to wait, but the overall throughput decreases. There are obvious tradeoffs and the method you choose depends on what your goal is, if you have multiple inputs that you want to do multiple operations on then pipelining is better, if you have a single input that you want to perform an operation on then the other option is better. To break circuits into stages and pipeline you must hold the input of the seperate stages stable, thus registers are used to break the circuit into seperate stages. This means an output will take multiple clock cycles to be determined, however, the total throughput for a bunch of inputs will be much shorter. The Latency of a K-Stage Pipeline is K*period of the clk. The Throughput of a K-Stage Pipeline is the frequency of the clk. For a well-formed K-Pipeline you must not mix the inputs. There is bottlenecks while pipelining components, and that would be your slowest component, because you can only go as fast as your slowest component will allow you to. To combat this you can either find a pipelined version of said component or you can interleave the component, the example given is interleaving a component with 2 copies, this in itself seems to act like a pipeline but it cuts the time delay for the component by the # of copies/stages of the pipelined component. The chapter then goes a bit over parellelism, in which you have two washers, 4 dryers, in this case you load in 2 inputs at a time, your throughput becomes even shorter 1 load every 15 minutes, however your latency is still 90 min per load. This doesnt mean that speedup can be infinite though, you are limited by timing overheads of all the registers and interleaving of components that you keep adding, causing you clock period to have a bound in turn making your throughput bounded as well. Seems like this is basically just a more detailed version of what we talked about in the focus group.

Chapter 14: This chapter is focused on Caches and Memory Hierachy. We just reviewed a bit of memory hierarchy in 362 as we talked about how microcontrollers read and write to GPIO's as memory. So we took a small look into the different types of memory. Anyway, the chapter initially talks about how the memory module is the most costly in their beta RISC computer. They call it a memory machine instead of a computing machine. There is basically a tradeoff, with registers access speed is super fast, but the amount of data you can hold in a register is in the 1000's of bits, meanwhile in DRAM and SRAM you can hold wayyy more. MB's and GB's of data, the tradeoff being that your access time significantly increases. The reasoning is increasing the number of bits increases area necessary for memory circuitry, leading to longer signal lines and longer times due to higher capacitive loads. So DRAM is lost cost and good in terms of capacity, but their access time is much greater, modern computers use a combination of SRAM and DRAM to try achieve a lower average latency, higher capacity memory hierarchy. Flash and hard-disk = Non-volatile, ie memory is preserved when power is shutdown. SRAM organization, array of memory cells, where you can read/write to all the bits in the cell. A row for each memory location and a column for each bit in a location. The cell for SRAM uses 6 mosfets to store and read/write a bit. To make this cheaper we can use 1 MOSFET to access the FET and a capacitor to store charge. This is a DRAM memory cell. The issue with this is that the capacitor will lose charge over time, thus why it is called "dynamic" memory. Quick comment/question here, if the capacitors are extremely close together could you potentially cause charges to affect other charges on the capacitor? Essentially intentionally switch bits without sending an instruction to actually do so? Just a random thought I had. When reading a bit, you have rewrite after every read in order to avoid losing data. You also have a refresh, where you have to read+write periodically in order to preserve data. This is 20x denser than SRAM but ~2-10x slower as a tradeoff. Won't write up too much on the non-volatiles as we didn't really cover them that much in focus group and we talked about how Registers, SRAM, DRAM, and caching were the main things we looked at. Memory Hierarchy, essentially 2 cases being described, one where you expose the hierarchy and tell the programmer that they have all these different memory storages and they get to decide in what way to use best use them, the other case was to hide all of it, tell the programmer that they got one large memory space they can access and have the hardware automatically optimize for what data is used more frequently to be put into the faster access memory modules. Locality principle, keep most often-used data in a small, fast SRAM local to the CPU. Refer to the main memory rarely whenever data needs to be accessed. Basically any SRAM block of data must be data that is frequently used, meaning that the memory system would need to predict what memory locations will be accessed. Caches, they exploit the locality reference principle, basically small interim storage that retains data from recently accessed locations. In memory hierarchy, everything is essentially a cache of something else- multilevel caching. Cache hit, in cache, send back quick. Cache miss, not in cache, transfer to cache by replacing something else. You can change block size, meaning you reduce the size of tag memory. Issue is that you will have few blocks in your cache. Have a bigger miss rate and penality as the block is larger to transfer to cache. Theres a bunch of different cache organization methods, the best way to determine which one is best is to simulate them all and figure out which works best for your purpose.

Chapter 15: This chapter is focused on pipelining a processor. The single cycle processor executes one instruction at a time going through all the steps before moving onto the next input. This is basically the laundry machine start where to do the load you do one load at a time for the whole washer-dryer system. Through the use of pipelining we can break this down into multiple seperate stages. IF, instruction fetch. RF, Register file. ALU/ or EX in our case when talking in the focus group, execute. MEM, memory access. WB, write back. Where each stage handles a different instruction at the same time. Basically one instruction per clock after the pipeline is filled, but for an instruction to get through to output it takes 5 clock cycles as it goes through every stage. Pipelining however, creates a bunch of hazards. Data hazards, where an instruction depends on the result of another which has not necessarily finished in the pipeline. Like if ADD produces a value that the next instruction needs, then the add value wont be ready yet. The solutions to this are either stalling where you pause or make the pipeline wait a cycle till the data is ready, or you use forwarding, where you pass the result directly instead of waiting for it to be written into a register file. The other issue would be control hazards. This is where you dont know what instruction to fetch when branching or jumping until the branching decision is made. You can once again stall until the branch is resolved but you can also try to create predictors to guess the outcome and keep the pipeline full, the cost being that if the predictor fails it will take even longer. Overall the chapter talked about how with a multistage pipeline CPU you get much better throughput overall, but when there are unexpected events such the data and control hazards noted, there must be specific operations in order to stop, drain,  or restart the pipeline. Making it much more complex to control than a single-cycle CPU. 

Overall Thoughts: I quite enjoyed learning more about pipeline staging, I understand a bit about memory but it is definetely not as clear as my understanding about pipelining. I believe I get the general gist and basic knowledge, but I feel as though it is just a bit more than surface level. I will continue to review these slides and ask questions on certain aspects I am unsure of as I go on. The focus group seemed to have answered a lot of my questions but I do think that the idea of reviewing these slides a bit more as I continue through next week would be a wise idea.